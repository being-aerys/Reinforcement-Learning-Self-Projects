{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The generation is  0\n",
      "best scores are  0.582\n",
      "best scores are  0.153\n",
      "best scores are  0.138\n",
      "best scores are  0.116\n",
      "best scores are  0.115\n",
      " The generation is  1\n",
      "best scores are  0.545\n",
      "best scores are  0.513\n",
      "best scores are  0.326\n",
      "best scores are  0.22\n",
      "best scores are  0.201\n",
      " The generation is  2\n",
      "best scores are  0.669\n",
      "best scores are  0.599\n",
      "best scores are  0.59\n",
      "best scores are  0.519\n",
      "best scores are  0.504\n",
      " The generation is  3\n",
      "best scores are  0.704\n",
      "best scores are  0.69\n",
      "best scores are  0.686\n",
      "best scores are  0.628\n",
      "best scores are  0.612\n",
      " The generation is  4\n",
      "best scores are  0.686\n",
      "best scores are  0.684\n",
      "best scores are  0.67\n",
      "best scores are  0.613\n",
      "best scores are  0.597\n",
      " The generation is  5\n",
      "best scores are  0.692\n",
      "best scores are  0.69\n",
      "best scores are  0.676\n",
      "best scores are  0.658\n",
      "best scores are  0.613\n",
      " The generation is  6\n",
      "best scores are  0.718\n",
      "best scores are  0.713\n",
      "best scores are  0.702\n",
      "best scores are  0.693\n",
      "best scores are  0.688\n",
      " The generation is  7\n",
      "best scores are  0.722\n",
      "best scores are  0.72\n",
      "best scores are  0.709\n",
      "best scores are  0.703\n",
      "best scores are  0.702\n",
      " The generation is  8\n",
      "best scores are  0.737\n",
      "best scores are  0.719\n",
      "best scores are  0.704\n",
      "best scores are  0.698\n",
      "best scores are  0.696\n",
      " The generation is  9\n",
      "best scores are  0.712\n",
      "best scores are  0.71\n",
      "best scores are  0.706\n",
      "best scores are  0.7\n",
      "best scores are  0.698\n",
      " The generation is  10\n",
      "best scores are  0.726\n",
      "best scores are  0.715\n",
      "best scores are  0.708\n",
      "best scores are  0.706\n",
      "best scores are  0.704\n",
      " The generation is  11\n",
      "best scores are  0.727\n",
      "best scores are  0.715\n",
      "best scores are  0.714\n",
      "best scores are  0.71\n",
      "best scores are  0.709\n",
      " The generation is  12\n",
      "best scores are  0.718\n",
      "best scores are  0.717\n",
      "best scores are  0.716\n",
      "best scores are  0.704\n",
      "best scores are  0.702\n",
      " The generation is  13\n",
      "best scores are  0.726\n",
      "best scores are  0.718\n",
      "best scores are  0.706\n",
      "best scores are  0.704\n",
      "best scores are  0.701\n",
      " The generation is  14\n",
      "best scores are  0.725\n",
      "best scores are  0.719\n",
      "best scores are  0.718\n",
      "best scores are  0.712\n",
      "best scores are  0.711\n",
      " The generation is  15\n",
      "best scores are  0.727\n",
      "best scores are  0.726\n",
      "best scores are  0.721\n",
      "best scores are  0.714\n",
      "best scores are  0.708\n",
      " The generation is  16\n",
      "best scores are  0.736\n",
      "best scores are  0.732\n",
      "best scores are  0.73\n",
      "best scores are  0.715\n",
      "best scores are  0.71\n",
      " The generation is  17\n",
      "best scores are  0.73\n",
      "best scores are  0.726\n",
      "best scores are  0.703\n",
      "best scores are  0.702\n",
      "best scores are  0.699\n",
      " The generation is  18\n",
      "best scores are  0.715\n",
      "best scores are  0.71\n",
      "best scores are  0.705\n",
      "best scores are  0.692\n",
      "best scores are  0.683\n",
      " The generation is  19\n",
      "best scores are  0.735\n",
      "best scores are  0.72\n",
      "best scores are  0.719\n",
      "best scores are  0.711\n",
      "best scores are  0.705\n",
      " The generation is  20\n",
      "best scores are  0.74\n",
      "best scores are  0.709\n",
      "best scores are  0.708\n",
      "best scores are  0.702\n",
      "best scores are  0.701\n",
      " The generation is  21\n",
      "best scores are  0.738\n",
      "best scores are  0.736\n",
      "best scores are  0.723\n",
      "best scores are  0.714\n",
      "best scores are  0.708\n",
      " The generation is  22\n",
      "best scores are  0.748\n",
      "best scores are  0.718\n",
      "best scores are  0.717\n",
      "best scores are  0.694\n",
      "best scores are  0.689\n",
      " The generation is  23\n",
      "best scores are  0.751\n",
      "best scores are  0.746\n",
      "best scores are  0.734\n",
      "best scores are  0.719\n",
      "best scores are  0.715\n",
      " The generation is  24\n",
      "best scores are  0.724\n",
      "best scores are  0.718\n",
      "best scores are  0.717\n",
      "best scores are  0.712\n",
      "best scores are  0.703\n",
      " The generation is  25\n",
      "best scores are  0.723\n",
      "best scores are  0.718\n",
      "best scores are  0.718\n",
      "best scores are  0.717\n",
      "best scores are  0.713\n",
      " The generation is  26\n",
      "best scores are  0.743\n",
      "best scores are  0.736\n",
      "best scores are  0.716\n",
      "best scores are  0.715\n",
      "best scores are  0.706\n",
      " The generation is  27\n",
      "best scores are  0.738\n",
      "best scores are  0.737\n",
      "best scores are  0.728\n",
      "best scores are  0.725\n",
      "best scores are  0.718\n",
      " The generation is  28\n",
      "best scores are  0.768\n",
      "best scores are  0.745\n",
      "best scores are  0.739\n",
      "best scores are  0.734\n",
      "best scores are  0.715\n",
      " The generation is  29\n",
      "best scores are  0.738\n",
      "best scores are  0.733\n",
      "best scores are  0.732\n",
      "best scores are  0.731\n",
      "best scores are  0.727\n",
      " The generation is  30\n",
      "best scores are  0.755\n",
      "best scores are  0.745\n",
      "best scores are  0.729\n",
      "best scores are  0.728\n",
      "best scores are  0.725\n",
      " The generation is  31\n",
      "best scores are  0.752\n",
      "best scores are  0.751\n",
      "best scores are  0.741\n",
      "best scores are  0.737\n",
      "best scores are  0.724\n",
      " The generation is  32\n",
      "best scores are  0.75\n",
      "best scores are  0.748\n",
      "best scores are  0.747\n",
      "best scores are  0.737\n",
      "best scores are  0.733\n",
      " The generation is  33\n",
      "best scores are  0.761\n",
      "best scores are  0.75\n",
      "best scores are  0.748\n",
      "best scores are  0.742\n",
      "best scores are  0.738\n",
      " The generation is  34\n",
      "best scores are  0.759\n",
      "best scores are  0.741\n",
      "best scores are  0.737\n",
      "best scores are  0.737\n",
      "best scores are  0.737\n",
      " The generation is  35\n",
      "best scores are  0.744\n",
      "best scores are  0.743\n",
      "best scores are  0.735\n",
      "best scores are  0.72\n",
      "best scores are  0.718\n",
      " The generation is  36\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-25f0e1d116c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_of_policies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mpolicy_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevaluate_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mno_of_episodes_for_each_policy\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m#Sort the scores and take the best 5 policies only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-25f0e1d116c1>\u001b[0m in \u001b[0;36mevaluate_policy\u001b[1;34m(env, policy, no_of_episodes_for_each_policy)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mtotal_expected_reward_after_all_episodes_for_a_policy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_of_episodes_for_each_policy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mtotal_expected_reward_after_all_episodes_for_a_policy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_timesteps_in_each_episode_during_training\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m#------------------------------------------VVI-------------------------------------------#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-25f0e1d116c1>\u001b[0m in \u001b[0;36mrun_episode\u001b[1;34m(env, policy, max_timesteps_in_each_episode_during_training, render)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m#implicitly thaha xa system lai ki 0,1,2,3 values ko menaing k ho bhanera\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#remember its env.step(), not obs.step()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mcumulative_reward_for_this_episode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcumulative_reward_for_this_episode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m\\\\depot.engr.oregonstate.edu\\users\\adhikara\\Windows.Documents\\My Documents\\Reinforcement Learning Personal Projects\\gym\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m\\\\depot.engr.oregonstate.edu\\users\\adhikara\\Windows.Documents\\My Documents\\Reinforcement Learning Personal Projects\\gym\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m\\\\depot.engr.oregonstate.edu\\users\\adhikara\\Windows.Documents\\My Documents\\Reinforcement Learning Personal Projects\\gym\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[1;34m(prob_n, np_random)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprob_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mcsprob_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import sys\n",
    "sys.path.append(r'\\\\depot.engr.oregonstate.edu\\users\\adhikara\\Windows.Documents\\My Documents\\Reinforcement Learning Personal Projects\\gym')\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "#print(np.random.choice(4,size = ((16))))\n",
    "\n",
    "def get_random_policy():\n",
    "    return(np.random.choice(4,size = ((16)))) \n",
    "    \n",
    "    #Generates a random sample from a given 1-D array\n",
    "    #If an int is passed as the first argument, the random sample is generated as if a were np.arange(a), here a = 4\n",
    "    #arrange([start,] stop[, step,][, dtype]) : Returns an array with evenly spaced elements as per the interval. \n",
    "    #only stop is the mandatory argument, here 4\n",
    "    #The interval mentioned is half opened i.e. [Start, Stop), so for us, 0 to 3\n",
    "    #size : int or tuple of ints, optional Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn.\n",
    "    #here 16 samples are drawn from the range [0,4)\n",
    "\n",
    "def run_episode(env, policy, max_timesteps_in_each_episode_during_training,render = False):   #render by default false set garne, jaba best policy bhetinxa, taba tetibela matra yo argument true banayera override garera call garne\n",
    "    #reset the observations in the environment before each episode\n",
    "    obs = env.reset()\n",
    "    cumulative_reward_for_this_episode = 0\n",
    "    for step in range(max_timesteps_in_each_episode_during_training):\n",
    "        \n",
    "        #lets render on the screen what happens when we take this action, but only when the render argument is true\n",
    "        if render:\n",
    "            \n",
    "            env.render()\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        #taking an action here is completely different from the cartpole problem\n",
    "        action = policy[obs] #pass the current position in the grid world as your arg\n",
    "        #starting position is always 0 after you reset the env\n",
    "        #policy has 0 to 15 values the tiles\n",
    "        #basically action gets the value stored at the 0th oposition cause we want an action for this positin\n",
    "        #logic : the value stored at the 0th position is in fact supposed to be the direction to take at this location in the environment\n",
    "        #implicitly thaha xa system lai ki 0,1,2,3 values ko menaing k ho bhanera\n",
    "        \n",
    "        obs, reward, done, _ = env.step(action) #remember its env.step(), not obs.step()\n",
    "        cumulative_reward_for_this_episode = cumulative_reward_for_this_episode + reward\n",
    "        if done == True:\n",
    "            break\n",
    "    return cumulative_reward_for_this_episode\n",
    "\n",
    "def evaluate_policy(env, policy, no_of_episodes_for_each_policy):\n",
    "    total_expected_reward_after_all_episodes_for_a_policy = 0\n",
    "    for i in range(no_of_episodes_for_each_policy):\n",
    "        total_expected_reward_after_all_episodes_for_a_policy += run_episode(env, policy, max_timesteps_in_each_episode_during_training )\n",
    "    \n",
    "    #------------------------------------------VVI-------------------------------------------#\n",
    "    #return expected reward, not the total sum of all episodes\n",
    "    \n",
    "    return (total_expected_reward_after_all_episodes_for_a_policy/ no_of_episodes_for_each_policy)\n",
    "    \n",
    "\n",
    "def crossover(elite_indivisual_1, elite_individual_2):\n",
    "    \n",
    "    #------------------------------VVI---------------------------------------------\n",
    "    #all value passes in python are pass-by-reference, so make sure you assign the values to a new variable before editing it\n",
    "    #DO NOT edit the passed value itself\n",
    "    new_policy = elite_indivisual_1.copy()\n",
    "    for i in range(16):\n",
    "        random_val = np.random.uniform()\n",
    "        if random_val > 0.5:\n",
    "            new_policy[i] = elite_individual_2[i]\n",
    "    return new_policy\n",
    "            \n",
    "\n",
    "def mutation(crossover_policy):\n",
    "    new_individual = crossover_policy.copy()\n",
    "    for inte in range (16):\n",
    "        random_value = np.random.uniform() #default range is 0 to 1\n",
    "        if random_value < mutation_probability_threshold_after_crossover:\n",
    "            new_individual[inte] = np.random.choice(4)\n",
    "    \n",
    "    return new_individual\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    no_of_policies = 100\n",
    "    no_of_generations = 100\n",
    "    max_timesteps_in_each_episode_during_training = 10000\n",
    "    policy_list = []\n",
    "    obs = env.reset()\n",
    "    policy_scores = np.random.rand(no_of_policies)\n",
    "    policy_scores_final = np.random.rand(no_of_policies)\n",
    "    no_of_episodes_for_each_policy = 1000\n",
    "    no_of_elite_policies = 5\n",
    "    crossover_offsprings = np.random.rand(no_of_policies-no_of_elite_policies,16)\n",
    "    mutated_offsprings = np.random.rand(no_of_policies-no_of_elite_policies,16)\n",
    "    elite_policies_list = np.random.rand(no_of_elite_policies,16)\n",
    "    mutation_probability_threshold_after_crossover = 0.3\n",
    "    \n",
    "    #generate_random_policies\n",
    "    for _ in range(no_of_policies):\n",
    "        policy_list.append(get_random_policy())\n",
    "    \n",
    "    #run every policy for a certain number of timesteps and evaluate\n",
    "    for steps in range(no_of_generations):\n",
    "        \n",
    "        print(\" The generation is \",steps)\n",
    "        #evaluate each policy for each generation\n",
    "        #print(\"no of policies \",no_of_policies )\n",
    "#       print(\"policy scores\", policy_scores)\n",
    "\n",
    "\n",
    "        for p in range(no_of_policies):\n",
    "            \n",
    "            policy_scores[p]=evaluate_policy(env, policy_list[p],no_of_episodes_for_each_policy )\n",
    "        \n",
    "        #Sort the scores and take the best 5 policies only\n",
    "        #print(\"policy scores are \",policy_scores)\n",
    "        descending_rank_of_scores_indices = list(reversed(np.argsort(policy_scores)))\n",
    "        #print(\"Descending order policy scores \", descending_rank_of_scores_indices)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        for x in range(5):\n",
    "            a = descending_rank_of_scores_indices[x]\n",
    "            print(\"best scores are \", policy_scores[a])\n",
    "            elite_policies_list[x] = policy_list[a]\n",
    "            \n",
    "        #print(\"Elite policies are \",elite_policies_list)\n",
    "            \n",
    "        \n",
    "        #elite_policies = policy_list[]\n",
    "        \n",
    "        probability_to_be_passed_on = policy_scores / np.sum(policy_scores)\n",
    "        \n",
    "        \n",
    "        #print(\"aa\",no_of_policies,probability_to_be_passed_on)\n",
    "        #lets crossover the best 5 organisms and replace the remaining with the new policies generated from the best 5\n",
    "         \n",
    "        for iter in range(no_of_policies-5):\n",
    "            crossover_offsprings[iter]=(crossover(policy_list[np.random.choice(range(no_of_policies), p = probability_to_be_passed_on)],\n",
    "                              policy_list[np.random.choice(range(no_of_policies), p = probability_to_be_passed_on)]))\n",
    "            \n",
    "        #here 5 ota bahek sab hamile crossover offsprings use garne ho next generation ma\n",
    "        \n",
    "        for iter in range(len(crossover_offsprings)):\n",
    "            mutated_offsprings[iter] = mutation(crossover_offsprings[iter]) \n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"Mutated_Offsprings are \", mutated_offsprings)\n",
    "        \n",
    "        #update the policy list as\n",
    "        \n",
    "        #print(\"Elite policies are  \",elite_policies_list, \"Mutated new policies are \",mutated_offsprings)\n",
    "        policy_list = np.concatenate((elite_policies_list, mutated_offsprings), axis = 0)\n",
    "        #print(\"Concatenaed policies after crossover \",policy_list)\n",
    "    \n",
    "    \n",
    "    #final_policies_scores\n",
    "    for iteration in range(no_of_policies):\n",
    "        policy_scores[iteration] = evaluate_policy(env, policy_list[iteration],no_of_episodes_for_each_policy)\n",
    "        \n",
    "        \n",
    "    print(\"policy scores final \",policy_scores)\n",
    "    index_of_best_policy_finally = np.argmax(policy_scores)#policy_list[np.argmax(policy_scores_final)]\n",
    "    \n",
    "    \n",
    "    print(\"Best Policy's scores at the end is \",policy_scores[index_of_best_policy_finally])\n",
    "    print(\"policy list is \",policy_list)\n",
    "    \n",
    "    print(\"The best policy is \",policy_list[index_of_best_policy_finally])\n",
    "    print(\"The best score is \",policy_scores[index_of_best_policy_finally])\n",
    "    \n",
    "    #lets run an episode and see for the best policy\n",
    "    policy_list = policy_list.astype(int)\n",
    "    #print(\"policy sores\",policy_scores)\n",
    "    \n",
    "    \n",
    "    run_episode(env, policy_list[index_of_best_policy_finally], max_timesteps_in_each_episode_during_training = 1000000000,render = True)\n",
    "    env.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "         \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #choose the best policies top\n",
    "    \n",
    "    #do genetic alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
