{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best policy is  (array([-0.13249988, -0.36209202,  0.52848676,  0.2085576 ]), 0.05876439242559961) and the corresponding score is  75.0\n"
     ]
    }
   ],
   "source": [
    "#Hello World\n",
    "# Good general-purpose agents don't need to know the semantics of the observations:\n",
    "# they can learn how to map observations to actions to maximize reward without any prior knowledge.\n",
    "# For cartpole balancing problem, it is \n",
    "# [position of cart, velocity of cart, angle of pole, rotation rate of pole]`. \n",
    "# Defined at https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py#L75\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(r'\\\\depot.engr.oregonstate.edu\\users\\adhikara\\Windows.Documents\\My Documents\\Reinforcement Learning Personal Projects\\gym')\n",
    "import gym\n",
    "#to list all envs\n",
    "# from gym import envs\n",
    "# print(envs.registry.all())\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#generate several plicies\n",
    "def get_policy():\n",
    "    return(np.random.uniform(-1,1,size = 4),np.random.uniform(-1,1)) # returns a tuple ([_,_,_,_], _)\n",
    "\n",
    "#take action from each policy\n",
    "def get_action_from_policy(obs, policy): #obs is the x values, env is the cartpole env, policy means the parameters\n",
    "    #print(\" obs and policy \",obs,policy)\n",
    "    if (np.dot(policy[0],obs+policy[1]) > 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#run individual episode for each policy\n",
    "def run_episode(env, policy, timesteps_max,render = False):   #render by default false set garne, jaba best policy bhetinxa, taba tetibela matra yo argument true banayera override garera call garne\n",
    "    #reset the observations in the environment before each episode\n",
    "    obs = env.reset()\n",
    "    cumulative_reward_for_this_episode = 0\n",
    "    for step in range(timesteps_max):\n",
    "        \n",
    "        #lets render on the screen what happens when we take this action, but only when the render argument is true\n",
    "        if render == True:\n",
    "            \n",
    "            env.render()\n",
    "            #time.sleep(0.03)\n",
    "        \n",
    "        #get the action for this policy for this env\n",
    "        action = get_action_from_policy(obs, policy)\n",
    "        obs, reward, done, _ = env.step(action) #remember its env.step(), not obs.step()\n",
    "        cumulative_reward_for_this_episode = cumulative_reward_for_this_episode + reward\n",
    "        if done == True:\n",
    "            break\n",
    "    return cumulative_reward_for_this_episode\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('CartPole-v0')\n",
    "    no_of_policies = 10\n",
    "    policy_list = []\n",
    "    scores_for_each_policy = []\n",
    "    timesteps_max = 1000\n",
    "    \n",
    "    #generate policies\n",
    "    for i in range(no_of_policies):\n",
    "        #print(\"i is \", i)\n",
    "        policy_list.append( get_policy())\n",
    "        \n",
    "    #run all episodes and store the rewards for each of them\n",
    "    for i in range (no_of_policies):\n",
    "        scores_for_each_policy.append(run_episode(env, policy_list[i], timesteps_max))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"The best policy is \",policy_list[np.argmax(scores_for_each_policy)], \"and the corresponding score is \",np.amax(scores_for_each_policy))\n",
    "    \n",
    "    #lets run the best policy\n",
    "    run_episode(env,policy_list[np.argmax(scores_for_each_policy)],100000,render = True )\n",
    "    env.close()\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"The total time taken was \",time.time()- start_time, \" seconds.\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
